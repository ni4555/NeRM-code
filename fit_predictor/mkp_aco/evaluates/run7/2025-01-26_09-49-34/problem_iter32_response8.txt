```python
import numpy as np

def heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:
    # Normalize prize by dividing each prize value by the sum of all prizes
    normalized_prize = prize / np.sum(prize)
    
    # Integrate domain knowledge: Consider items with a higher ratio of prize to weight as more promising
    ratio_factor = normalized_prize / weight.sum(axis=1)
    
    # Promote diversity and control variance by calculating a dynamic factor based on the standard deviation
    std_factor = np.mean(np.std(weight, axis=1))
    
    # Balance the factors: give more weight to high prize-to-weight ratio and controlled variance
    combined_factor = ratio_factor * 0.6 + std_factor * 0.4
    
    # Introduce exploration and exploitation balance by adjusting the factor for each item
    exploration_exploitation_factor = np.random.rand(*combined_factor.shape) * 0.1
    
    # Adjust the heuristics based on the combination of factors and exploration-exploitation balance
    heuristics = combined_factor * (1 + exploration_exploitation_factor)
    
    # Refine heuristics iteratively to balance exploration and exploitation
    previous_heuristics = heuristics.copy()
    for _ in range(3):
        weighted_sum = heuristics * (heuristics / previous_heuristics)
        previous_heuristics = heuristics
        heuristics = weighted_sum / np.sum(weighted_sum)
    
    # Normalize heuristics to prevent overflow and introduce sparsity
    heuristics /= np.sum(heuristics)
    threshold = np.mean(heuristics)
    heuristics[heuristics < threshold] = 0
    
    # Apply a feedback mechanism that adjusts heuristics based on their performance
    performance_factor = np.mean(prize[heuristics > 0]) / np.mean(prize)
    heuristics *= performance_factor
    
    # Sparsify the heuristics by setting a fraction of the lowest heuristics to zero
    num_to_zero = int(len(heuristics) * 0.05)
    heuristics[np.argsort(heuristics)[:num_to_zero]] = 0
    
    return heuristics
```
