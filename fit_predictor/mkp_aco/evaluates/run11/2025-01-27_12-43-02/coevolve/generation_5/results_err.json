{
  "generation": 5,
  "description": "Develop a hybrid heuristic leveraging a probabilistic Genetic Algorithm (GA) for initial item subset exploration combined with a sophisticated Stochastic Local Search (SLS) algorithm to iteratively refine the solution space, ultimately maximizing total prize accumulation while ensuring compliance with stringent weight limitations across a multi-knapsack setup.",
  "failed_solutions": [
    {
      "code": "import numpy as np\nimport numpy as np\n\ndef heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:\n    n, m = prize.shape[0], weight.shape[1]\n    \n    # Probabilistic Genetic Algorithm (GA) for initial exploration\n    population_size = 100\n    generations = 20\n    mutation_rate = 0.01\n    \n    # Initialize population\n    population = np.random.randint(0, 2, (population_size, n))\n    \n    # GA main loop\n    for _ in range(generations):\n        # Evaluate fitness\n        fitness = np.sum(prize * population, axis=1)\n        \n        # Selection\n        sorted_indices = np.argsort(fitness)[::-1]\n        population = population[sorted_indices[:population_size // 2]]\n        \n        # Crossover\n        for i in range(0, population_size, 2):\n            idx1, idx2 = sorted_indices[i], sorted_indices[i+1]\n            child1, child2 = population[idx1].copy(), population[idx2].copy()\n            for j in range(n):\n                if np.random.rand() < 0.5:\n                    child1[j], child2[j] = child2[j], child1[j]\n            population[i], population[i+1] = child1, child2\n        \n        # Mutation\n        for i in range(population_size):\n            for j in range(n):\n                if np.random.rand() < mutation_rate:\n                    population[i][j] = 1 - population[i][j]\n    \n    # Initial heuristics from GA\n    ga_heuristics = np.sum(prize * population[0], axis=1)\n    \n    # Stochastic Local Search (SLS) to refine the solution\n    max_iterations = 100\n    step_size = 0.1\n    \n    # Initial random solution\n    current_solution = np.random.randint(0, 2, n)\n    current_prize = np.sum(prize * current_solution)\n    \n    for _ in range(max_iterations):\n        # Neighbourhood search\n        for _ in range(int(n * step_size)):\n            i = np.random.randint(n)\n            neighbour = current_solution.copy()\n            neighbour[i] = 1 - neighbour[i]\n            if np.sum(weight * neighbour, axis=1).max() <= 1:\n                if np.sum(prize * neighbour) > current_prize:\n                    current_solution, current_prize = neighbour, np.sum(prize * neighbour)\n        \n        # Update heuristics based on refined solution\n        ga_heuristics[current_solution] = current_prize\n    \n    return ga_heuristics",
      "fitness": Infinity,
      "execution_success": false,
      "error": "Filtered by predictor (rank: 2, confidence: 0.9982)",
      "stdout_file": null,
      "code_file": null
    },
    {
      "code": "import numpy as np\nimport numpy as np\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef genetic_algorithm(prize: np.ndarray, weight: np.ndarray):\n    n, m = prize.shape[0], weight.shape[1]\n    population_size = 100\n    generations = 20\n    crossover_rate = 0.8\n    mutation_rate = 0.02\n    \n    # Initial population\n    population = np.random.randint(2, size=(population_size, n))\n    \n    def fitness(individual):\n        total_weight = np.sum(individual * weight, axis=1)\n        return np.sum(individual * prize) if np.all(total_weight <= 1) else 0\n    \n    def crossover(parent1, parent2):\n        if np.random.rand() < crossover_rate:\n            crossover_point = np.random.randint(n)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        return parent1, parent2\n    \n    def mutate(individual):\n        for i in range(n):\n            if np.random.rand() < mutation_rate:\n                individual[i] = 1 - individual[i]\n        return individual\n    \n    for _ in range(generations):\n        fitness_scores = np.array([fitness(ind) for ind in population])\n        selected_indices = np.argsort(fitness_scores)[-population_size // 2:]\n        new_population = population[selected_indices]\n        for i in range(0, population_size, 2):\n            parent1, parent2 = population[np.random.choice(selected_indices, 2)]\n            child1, child2 = crossover(parent1, parent2)\n            new_population[i] = mutate(child1)\n            if i + 1 < population_size:\n                new_population[i + 1] = mutate(child2)\n        population = new_population\n    \n    best_individual = population[np.argmax(fitness_scores)]\n    return best_individual\n\ndef stochastic_local_search(best_individual: np.ndarray, prize: np.ndarray, weight: np.ndarray):\n    max_iterations = 1000\n    iteration = 0\n    while iteration < max_iterations:\n        candidate = np.random.choice([0, 1], size=best_individual.shape)\n        if np.all(np.sum(candidate * weight, axis=1) <= 1):\n            if np.sum(candidate * prize) > np.sum(best_individual * prize):\n                best_individual = candidate\n        iteration += 1\n    return best_individual\n\ndef heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:\n    best_individual = genetic_algorithm(prize, weight)\n    best_individual = stochastic_local_search(best_individual, prize, weight)\n    return best_individual",
      "fitness": Infinity,
      "execution_success": false,
      "error": "Filtered by predictor (rank: 3, confidence: 0.9978)",
      "stdout_file": null,
      "code_file": null
    },
    {
      "code": "import numpy as np\nimport numpy as np\nfrom scipy.optimize import differential_evolution\n\ndef fitness_func(individual, prize, weight):\n    subset = [i for i, value in enumerate(individual) if value > 0.5]\n    total_prize = sum(prize[i] for i in subset)\n    total_weight = sum(weight[i][0] for i in subset)\n    if total_weight > 1:\n        return 0\n    return total_prize\n\ndef stochastic_local_search(prize, weight, initial_subset):\n    best_solution = initial_subset\n    best_fitness = fitness_func(initial_subset, prize, weight)\n    \n    while True:\n        # Perform some form of local search\n        # For example, swap two elements in the subset\n        index_1, index_2 = np.random.choice(len(prize), 2, replace=False)\n        new_subset = best_solution.copy()\n        new_subset[index_1], new_subset[index_2] = new_subset[index_2], new_subset[index_1]\n        \n        new_fitness = fitness_func(new_subset, prize, weight)\n        if new_fitness > best_fitness:\n            best_solution = new_subset\n            best_fitness = new_fitness\n        \n        # Decide whether to stop or continue the search\n        if np.random.rand() < np.exp((new_fitness - best_fitness) / best_fitness):\n            break\n    \n    return best_solution\n\ndef heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:\n    # Use differential evolution to perform the initial exploration\n    bounds = [(0, 1)] * len(prize)\n    result = differential_evolution(lambda x: -fitness_func(x, prize, weight), bounds)\n    \n    # Use the best individual from DE as the initial subset for SLS\n    initial_subset = [1 if value > 0.5 else 0 for value in result.x]\n    \n    # Use SLS to refine the solution\n    refined_subset = stochastic_local_search(prize, weight, initial_subset)\n    \n    # Return the refined subset\n    return np.array(refined_subset)",
      "fitness": Infinity,
      "execution_success": false,
      "error": "Filtered by predictor (rank: 4, confidence: 0.9967)",
      "stdout_file": null,
      "code_file": null
    },
    {
      "code": "import numpy as np\nimport numpy as np\nfrom scipy.stats import norm\n\ndef heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:\n    n, m = prize.shape[0], weight.shape[1]\n    \n    # Genetic Algorithm parameters\n    population_size = 100\n    generations = 50\n    mutation_rate = 0.1\n    \n    # Initialize population\n    population = np.random.rand(population_size, n)\n    population = (population > 0.5).astype(int)\n    \n    # Fitness function\n    def fitness(individual):\n        total_prize = np.dot(individual, prize)\n        total_weight = np.dot(individual, weight)\n        return total_prize, total_weight\n    \n    # Genetic operators\n    def crossover(parent1, parent2):\n        child = np.random.rand(n)\n        child[np.random.choice(n, 2, replace=False)] = parent1[np.random.choice(n, 2, replace=False)]\n        child[np.random.choice(n, 2, replace=False)] = parent2[np.random.choice(n, 2, replace=False)]\n        return (child > 0.5).astype(int)\n    \n    def mutate(individual):\n        individual[np.random.choice(n)] = 1 - individual[np.random.choice(n)]\n        return individual\n    \n    # Evolution\n    for generation in range(generations):\n        population_fitness = np.array([fitness(individual) for individual in population])\n        population = population[population_fitness[:, 0].argsort()[::-1]]\n        \n        new_population = []\n        while len(new_population) < population_size:\n            parent1, parent2 = population[np.random.choice(population_size, 2, replace=False)]\n            child = crossover(parent1, parent2)\n            if np.random.rand() < mutation_rate:\n                child = mutate(child)\n            new_population.append(child)\n        population = np.array(new_population)\n    \n    # SLS algorithm\n    def stochastic_local_search(individual):\n        for _ in range(100):\n            neighbor = np.copy(individual)\n            index = np.random.randint(n)\n            neighbor[index] = 1 - neighbor[index]\n            neighbor_fitness, _ = fitness(neighbor)\n            if neighbor_fitness > fitness(individual)[0]:\n                individual = neighbor\n        return individual\n    \n    best_individual = population_fitness[:, 0].argmax()\n    best_solution = stochastic_local_search(population[best_individual])\n    \n    # Calculate heuristic scores\n    heuristic_scores = np.zeros(n)\n    for i in range(n):\n        heuristic_scores[i] = norm.cdf(-np.log(1 - np.sum(best_solution[:i+1] * prize[:i+1]) / np.sum(best_solution[:i+1] * weight[:i+1])))\n    \n    return heuristic_scores",
      "fitness": Infinity,
      "execution_success": false,
      "error": "Traceback (most recent call last):\n  File \"E:\\Projects\\CO\\reevo-main/problems/mkp_aco/eval.py\", line 58, in <module>\n    obj = solve(prize, weight)\n          ^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Projects\\CO\\reevo-main/problems/mkp_aco/eval.py\", line 23, in solve\n    heu = heuristics(prize.copy(), weight.copy()) + 1e-9\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Projects\\CO\\reevo-main\\problems\\mkp_aco\\gpt.py\", line 36, in heuristics_v2\n    population_fitness = np.array([fitness(individual) for individual in population])\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (100, 2) + inhomogeneous part.\n",
      "stdout_file": "coevolve\\generation_5\\stdout_1.txt",
      "code_file": "coevolve\\generation_5\\code_1.py"
    }
  ]
}