{
  "generation": 8,
  "description": "Designing an optimization heuristic for the Multi-Knapsack Problem (MKP) that incorporates stochastic solution sampling and ensemble learning. The heuristic should leverage a combination of Genetic Algorithms (GAs) and Stochastic Local Search (SLS) algorithms to identify an optimal subset of items for each knapsack, aiming to maximize the total prize while respecting the weight constraints. The approach should prioritize clarity in algorithm design and implementation, utilizing adaptive sampling techniques to explore diverse solution spaces effectively.",
  "failed_solutions": [
    {
      "code": "import numpy as np\nimport numpy as np\n\ndef fitness(solution, prize, weight, knapsack_capacity):\n    total_weight = sum(weight[solution > 0])\n    if total_weight <= knapsack_capacity:\n        return sum(prize[solution > 0])\n    else:\n        return 0\n\ndef genetic_algorithm(prize, weight, population_size=50, max_iterations=100, knapsack_capacity=1):\n    n, m = prize.shape[0], weight.shape[1]\n    population = np.random.randint(2, size=(population_size, n))\n    best_fitness = 0\n    best_solution = np.zeros(n)\n\n    for _ in range(max_iterations):\n        fitness_values = np.array([fitness(sol, prize, weight, knapsack_capacity) for sol in population])\n        if np.max(fitness_values) > best_fitness:\n            best_fitness = np.max(fitness_values)\n            best_solution = population[np.argmax(fitness_values)]\n\n        new_population = np.copy(population)\n        for i in range(population_size // 2):\n            parent1 = population[np.argmax(fitness_values)]\n            parent2 = population[np.argmax(fitness_values)]\n            child1 = np.random.choice([0, 1], size=n, p=[0.5, 0.5])\n            child2 = np.random.choice([0, 1], size=n, p=[0.5, 0.5])\n            cross_point = np.random.randint(1, n)\n            child1[:cross_point] = parent2[:cross_point]\n            child2[:cross_point] = parent1[:cross_point]\n            new_population[2*i] = child1\n            new_population[2*i+1] = child2\n\n        population = new_population\n\n    return best_solution\n\ndef stochastic_local_search(prize, weight, solution, knapsack_capacity):\n    while True:\n        neighbor = np.copy(solution)\n        for i in range(solution.shape[0]):\n            if np.random.rand() < 0.5:\n                neighbor[i] = 1 - neighbor[i]\n        neighbor_fitness = fitness(neighbor, prize, weight, knapsack_capacity)\n        solution_fitness = fitness(solution, prize, weight, knapsack_capacity)\n        if neighbor_fitness >= solution_fitness:\n            solution = neighbor\n        else:\n            break\n\ndef heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:\n    n, m = prize.shape[0], weight.shape[1]\n    knapsack_capacity = 1\n    population_size = 50\n    max_iterations = 100\n    initial_solution = genetic_algorithm(prize, weight, population_size, max_iterations, knapsack_capacity)\n    stochastic_local_search(prize, weight, initial_solution, knapsack_capacity)\n    heuristic_values = np.exp(fitness(initial_solution, prize, weight, knapsack_capacity))\n    heuristic_values /= np.sum(heuristic_values)\n    return heuristic_values",
      "fitness": Infinity,
      "execution_success": false,
      "error": "Filtered by predictor (rank: 2, confidence: 0.9975)",
      "stdout_file": null,
      "code_file": null
    },
    {
      "code": "import numpy as np\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\ndef heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:\n    n, m = weight.shape\n    features = weight.reshape(-1, 1)\n    labels = np.where(np.cumsum(prize) > np.cumsum(prize[-1]), 1, 0)\n    \n    # Train a random forest classifier\n    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n    classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n    classifier.fit(X_train, y_train)\n    \n    # Predict probabilities for each item\n    probabilities = classifier.predict_proba(X_test)[:, 1]\n    \n    # Normalize probabilities to sum to 1\n    probabilities /= probabilities.sum()\n    \n    # Sample from the probability distribution to get heuristic values\n    heuristics = np.random.choice([0, 1], size=n, p=probabilities)\n    \n    return heuristics",
      "fitness": Infinity,
      "execution_success": false,
      "error": "Filtered by predictor (rank: 3, confidence: 0.9966)",
      "stdout_file": null,
      "code_file": null
    },
    {
      "code": "import numpy as np\nimport numpy as np\n\ndef heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:\n    n, m = prize.shape[0], weight.shape[1]\n    population_size = 50\n    crossover_rate = 0.8\n    mutation_rate = 0.2\n    number_of_ensembles = 5\n\n    # Helper functions for Genetic Algorithm\n    def crossover(parent1, parent2):\n        if np.random.rand() < crossover_rate:\n            idx = np.random.randint(0, len(parent1))\n            child1 = np.concatenate((parent1[:idx], parent2[idx:]))\n            child2 = np.concatenate((parent2[:idx], parent1[idx:]))\n            return child1, child2\n        else:\n            return parent1, parent2\n\n    def mutate(individual):\n        for i in range(len(individual)):\n            if np.random.rand() < mutation_rate:\n                individual[i] = 1 - individual[i]\n        return individual\n\n    def genetic_algorithm():\n        population = np.random.randint(0, 2, (population_size, n))\n        fitness_scores = []\n        for generation in range(100):  # number of generations\n            fitness_scores = [np.dot(individual, np.maximum(0, np.dot(individual, weight))) - np.sum(individual) for individual in population]\n            # SLS\n            while np.sum(fitness_scores > 0):\n                worst = np.argmin(fitness_scores)\n                best = np.argmax([np.dot(i, np.maximum(0, np.dot(i, weight))) - np.sum(i) for i in population if i != population[worst]])\n                population[worst], population[best] = population[best], population[worst]\n                fitness_scores[worst], fitness_scores[best] = fitness_scores[best], fitness_scores[worst]\n            fitness_scores = np.clip(fitness_scores, 0, 1)\n            new_population = []\n            while len(new_population) < population_size:\n                parent1, parent2 = population[np.random.choice(range(population_size), 2, replace=False)]\n                child1, child2 = crossover(parent1, parent2)\n                child1 = mutate(child1)\n                child2 = mutate(child2)\n                new_population.extend([child1, child2])\n            population = np.array(new_population[:population_size])\n        best_individual = population[np.argmax(fitness_scores)]\n        return best_individual\n\n    # Ensemble learning\n    ensemble = []\n    for _ in range(number_of_ensembles):\n        ensemble.append(genetic_algorithm())\n\n    # Adaptive sampling\n    samples = np.array([np.mean(ensemble, axis=0), np.random.choice(ensemble).flatten()])\n    heuristic_values = np.maximum(0, np.dot(samples, np.dot(prize, weight)) - np.sum(samples))\n\n    return heuristic_values",
      "fitness": Infinity,
      "execution_success": false,
      "error": "Filtered by predictor (rank: 4, confidence: 0.9958)",
      "stdout_file": null,
      "code_file": null
    },
    {
      "code": "import numpy as np\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\ndef heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:\n    n, m = weight.shape\n    population_size = 100\n    generations = 50\n    mutation_rate = 0.1\n    elite_size = 5\n    max_weight = np.sum(weight, axis=1).max()\n    \n    # Initialize population\n    population = np.random.randint(2, size=(population_size, n))\n    \n    # Genetic Algorithm\n    for _ in range(generations):\n        # Evaluate fitness\n        fitness = np.dot(population, prize)\n        \n        # Selection\n        fitness = 1 / (1 + fitness)  # Normalize fitness\n        fitness /= np.sum(fitness)\n        selected_indices = np.random.choice(population_size, size=population_size, p=fitness)\n        selected_population = population[selected_indices]\n        \n        # Crossover\n        offspring = np.empty((population_size, n))\n        for i in range(0, population_size, 2):\n            parent1, parent2 = selected_population[i], selected_population[i+1]\n            crossover_point = np.random.randint(n)\n            offspring[i] = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            offspring[i+1] = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n        \n        # Mutation\n        for i in range(population_size):\n            if np.random.rand() < mutation_rate:\n                mutation_point = np.random.randint(n)\n                offspring[i][mutation_point] = 1 - offspring[i][mutation_point]\n        \n        # Replace old population with offspring\n        population = offspring\n    \n    # Stochastic Local Search\n    best_solution = population[np.argmax(np.dot(population, prize))]\n    for _ in range(10):  # Number of iterations for SLS\n        for i in range(n):\n            if np.random.rand() < 0.5:\n                best_solution[i] = 1 - best_solution[i]\n            else:\n                best_solution[i] = np.random.randint(2)\n            if np.dot(best_solution, weight) <= max_weight:\n                break\n    \n    # Ensemble Learning\n    X_train, X_test, y_train, y_test = train_test_split(population, np.dot(population, prize), test_size=0.2, random_state=42)\n    classifier = RandomForestClassifier(n_estimators=10)\n    classifier.fit(X_train, y_train)\n    predictions = classifier.predict(X_test)\n    heuristic_values = classifier.predict(np.tile(best_solution, (X_test.shape[0], 1)))\n    \n    return heuristic_values",
      "fitness": Infinity,
      "execution_success": false,
      "error": "Filtered by predictor (rank: 5, confidence: 0.8101)",
      "stdout_file": null,
      "code_file": null
    },
    {
      "code": "import numpy as np\nimport numpy as np\nimport random\n\ndef select_parent(population):\n    fitness_sum = sum(individual['fitness'] for individual in population)\n    total = random.uniform(0, fitness_sum)\n    current_sum = 0\n    for individual in population:\n        current_sum += individual['fitness']\n        if current_sum > total:\n            return individual\n\ndef crossover(parent1, parent2):\n    child = []\n    for gene1, gene2 in zip(parent1, parent2):\n        if random.random() < 0.5:\n            child.append(gene1)\n        else:\n            child.append(gene2)\n    return child\n\ndef mutate(child):\n    for i in range(len(child)):\n        if random.random() < 0.1:\n            child[i] = 1 - child[i]\n    return child\n\ndef genetic_algorithm(prize, weight):\n    population_size = 100\n    generations = 100\n    mutation_rate = 0.1\n    crossover_rate = 0.9\n    \n    population = [{'chromosome': [random.choice([0, 1]) for _ in range(prize.size)],\n                   'fitness': 0} for _ in range(population_size)]\n    \n    for individual in population:\n        individual['fitness'] = sum(prize[i] if weight[i, 0] <= 1 else 0 for i in range(prize.size) if individual['chromosome'][i])\n    \n    for _ in range(generations):\n        new_population = []\n        for _ in range(population_size // 2):\n            parent1 = select_parent(population)\n            parent2 = select_parent(population)\n            child = crossover(parent1['chromosome'], parent2['chromosome'])\n            child = mutate(child)\n            child = np.array(child)\n            new_population.append({'chromosome': child, 'fitness': 0})\n        \n        for individual in new_population:\n            individual['fitness'] = sum(prize[i] if weight[i, 0] <= 1 else 0 for i in range(prize.size) if individual['chromosome'][i])\n        \n        population = new_population\n    \n    best_individual = max(population, key=lambda x: x['fitness'])\n    return best_individual['chromosome']\n\ndef stochastic_local_search(best_individual):\n    current_individual = best_individual.copy()\n    for _ in range(1000):\n        random_index = random.randint(0, current_individual.size - 1)\n        if random.random() < 0.5:\n            current_individual[random_index] = 1 - current_individual[random_index]\n        new_fitness = sum(prize[i] if weight[i, 0] <= 1 else 0 for i in range(prize.size) if current_individual[i])\n        if new_fitness > current_individual['fitness']:\n            current_individual['fitness'] = new_fitness\n    \n    return current_individual\n\ndef heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:\n    best_individual = genetic_algorithm(prize, weight)\n    optimized_individual = stochastic_local_search(best_individual)\n    return optimized_individual",
      "fitness": Infinity,
      "execution_success": false,
      "error": "Traceback (most recent call last):\n  File \"E:\\Projects\\CO\\reevo-main/problems/mkp_aco/eval.py\", line 58, in <module>\n    obj = solve(prize, weight)\n          ^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Projects\\CO\\reevo-main/problems/mkp_aco/eval.py\", line 23, in solve\n    heu = heuristics(prize.copy(), weight.copy()) + 1e-9\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Projects\\CO\\reevo-main\\problems\\mkp_aco\\gpt.py\", line 73, in heuristics_v2\n    optimized_individual = stochastic_local_search(best_individual)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Projects\\CO\\reevo-main\\problems\\mkp_aco\\gpt.py\", line 65, in stochastic_local_search\n    new_fitness = sum(prize[i] if weight[i, 0] <= 1 else 0 for i in range(prize.size) if current_individual[i])\n                                                                          ^^^^^\nNameError: name 'prize' is not defined. Did you mean: 'print'?\n",
      "stdout_file": "coevolve\\generation_8\\stdout_4.txt",
      "code_file": "coevolve\\generation_8\\code_4.py"
    }
  ]
}