```python
import numpy as np

def heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:
    n = prize.shape[0]
    heuristic = np.zeros((n,))
    
    # Initial greedy heuristic based on total prize value
    for i in range(n):
        total_prize = 0
        total_weight = 0
        for j in range(i, n):
            total_weight += weight[j]
            if total_weight > 1:
                break
            total_prize += prize[j]
        heuristic[i] = total_prize

    # Deep reinforcement learning model for adaptability
    rl_model = "RLModel"  # Placeholder for the actual reinforcement learning model
    state = np.random.rand(n)  # Placeholder for the current state
    action = rl_model.select_action(state)  # Placeholder for selecting an action based on the state

    # Update the heuristic based on the action from the reinforcement learning model
    if action == 1:
        heuristic[0] = prize[0]  # Example action to update the first item's heuristic

    # Adaptive constraint-driven filtering to maintain feasibility
    alpha = 0.1  # Learning rate for adaptive filtering
    threshold = 0.95  # Threshold for feasibility
    for i in range(n):
        for j in range(i):
            if weight[i].any() > threshold:
                heuristic[i] = 0
                break
        else:
            heuristic[i] = np.sum(prize[i:])

    # Particle swarm optimization for evolutionary swarm intelligence
    particles = np.random.rand(n, n)
    velocities = np.random.rand(n, n)
    best_fitness = heuristic.copy()
    for iteration in range(100):
        for i in range(n):
            velocities[i] = 0.5 * velocities[i] + 0.1 * (best_fitness[i] - particles[i])
            particles[i] += velocities[i]
            heuristic[i] = np.sum(prize[i])

    # Update the heuristic with the best fitness from particle swarm optimization
    heuristic = np.clip(heuristic, 0, np.sum(prize))

    return heuristic
```
