```python
import numpy as np

def heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:
    n = prize.shape[0]
    m = weight.shape[1]
    heuristic = np.zeros((n,))

    # Deep Reinforcement Learning (DRL) for adaptive decision-making
    model_scores = np.random.rand(n)
    def adaptive_reward_function(x, prev_reward, step_size=0.1):
        reward_diff = np.sum(x * np.log(1 + x)) - prev_reward
        return prev_reward + step_size * reward_diff
    prev_reward = adaptive_reward_function(model_scores)
    model_rewards = [prev_reward]

    # Adaptive Constraint-Driven Filtering to maintain feasibility
    feasible_items = np.sum(weight, axis=1) <= 1

    # Particle Swarm Optimization (PSO) for swarm intelligence
    pso_position = np.random.rand(n)
    pso_velocity = np.zeros(n)
    pso_personal_best = np.copy(pso_position)
    pso_global_best = np.copy(pso_position)
    for _ in range(5):  # Sufficiently smaller iterations to simulate a real-time heuristic
        for i in range(n):
            if i in feasible_items:  # Only consider feasible items
                pso_velocity[i] = 0.5 * (np.random.rand() - 0.5) + pso_velocity[i]
                pso_position[i] += pso_velocity[i]
                pso_position[i] = np.clip(pso_position[i], 0, 1)

        current_pso_scores = pso_position.copy()
        for i in range(n):
            if i in feasible_items:
                if adaptive_reward_function(current_pso_scores[i], prev_reward) > prev_reward:
                    pso_global_best = np.copy(pso_position)
                    prev_reward = adaptive_reward_function(current_pso_scores[i], prev_reward)

    # Balancing exploitation and exploration
    global_best_index = np.argmax(prev_reward)
    exploration_exploit_ratio = np.random.rand()

    # Final heuristic is influenced by the exploration_exploit_ratio
    heuristic = np.full(n, 1e-6)  # Default very low score
    if exploration_exploit_ratio < 0.5:  # Exploitation
        heuristic[feasible_items] = prize[global_best_index] / prize
    else:  # Exploration
        for i in range(n):
            if i in feasible_items:
                heuristic[i] = prize[i] / prize[global_best_index]

    # Sparsify heuristics with adaptive sparsity threshold
    sparsity_threshold = np.percentile(heuristic[feasible_items], 20)
    heuristic[heuristic < sparsity_threshold] = 0

    return heuristic
```
