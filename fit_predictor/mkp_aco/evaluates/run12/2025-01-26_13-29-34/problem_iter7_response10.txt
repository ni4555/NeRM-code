```python
import numpy as np

def heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:

    n = prize.shape[0]
    m = weight.shape[1]
    heuristic = np.zeros((n,))

    # Simulate a pre-trained deep reinforcement learning model
    model_scores = np.random.rand(n)

    # Initialize adaptive constraint-driven filtering
    feasibility_flags = np.zeros(n, dtype=bool)

    # Check initial feasibility across dimensions
    for dim in range(m):
        for i in range(n):
            if weight[i, dim] > 1:
                feasibility_flags[i] = False
                break

    # Initialize PSO variables
    particles = np.copy(model_scores)  # Start with the scores as initial particles
    velocities = np.zeros_like(particles)
    global_best_index = np.argmax(model_scores)
    global_best_score = np.max(model_scores)

    # Adaptive parameters for PSO
    w = 0.5  # Inertia weight
    c1, c2 = 2.0, 2.0  # Cognitive and social coefficients

    # Run PSO for a few iterations to refine heuristics
    for _ in range(5):  # Assuming 5 iterations for demonstration
        for i in range(n):
            velocities[i] = w * velocities[i] + c1 * np.random.rand() * (model_scores[i] - particles[i]) + c2 * np.random.rand() * (global_best_score - particles[i])
            particles[i] += velocities[i]

        # Update global best
        current_best_score = np.max(particles)
        if current_best_score > global_best_score:
            global_best_index = np.argmax(particles)
            global_best_score = current_best_score

    # Apply adaptive constraint-driven filtering
    for i in range(n):
        feasible = feasibility_flags[i] and (weight[i] <= np.array([1] * m))
        if not feasible:
            heuristic[i] = 0
        else:
            heuristic[i] = particles[i] * (prize[i] / (prize[global_best_index] + 1e-6))

    # Sparsify the heuristic
    threshold = 0.1
    heuristic[heuristic < threshold] = 0

    return heuristic
```
