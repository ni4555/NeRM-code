```python
def heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:
    n, m = prize.shape[0], weight.shape[1]
    heuristic = np.zeros((prize.shape[0],))

    # Deep reinforcement learning component (simplified)
    model_scores = np.random.rand(prize.shape[0])

    # Pre-compute cumulative prize and weight to minimize feasibility checks
    cumulative_prize = np.zeros((n,))
    cumulative_weight = np.zeros((n,))
    cumulative_prize[0] = prize[0]
    cumulative_weight[0] = weight[0][0]
    for i in range(1, n):
        cumulative_prize[i] = cumulative_prize[i - 1] + prize[i]
        cumulative_weight[i] = cumulative_weight[i - 1] + weight[i][0]

    # Particle swarm optimization for evolutionary swarm intelligence
    for i in range(n):
        for j in range(m):
            # Update local and global best positions based on dimension j
            local_best = np.max(prize[i:])
            global_best = np.max(prize)
            
            # Calculate heuristic based on the factors
            heuristic[i] = (model_scores[i] * prize[i] * (1 - cumulative_weight[i]) / (local_best + 1e-6) *
                            ((global_best - prize[i]) / (global_best + 1e-6)))

    # Use domain knowledge to reduce feasibility checks
    for i in range(n):
        # Since constraint is 1, only need to check up to item n
        while i < n and cumulative_weight[i] <= 1:
            if cumulative_prize[i] - cumulative_prize[i - 1] > 0:  # Only items that add value are considered
                heuristic[i] = model_scores[i] * (cumulative_prize[i] - cumulative_prize[i - 1]) * (1 / (local_best + 1e-6))
            i += 1

    # Combine multiple heuristics for complementary strengths
    # Here we are simply taking the weighted average of model_scores and heuristic values
    heuristic = model_scores * heuristic

    return heuristic
```
