```python
import numpy as np
import random

def heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:
    m = weight.shape[1]
    heuristic = np.zeros((prize.shape[0],))

    # Deep Reinforcement Learning for Real-Time Decision-Making
    # Initialize Q-table (placeholder for demonstration purposes)
    Q_table = np.zeros((prize.shape[0], 2))  # 2 actions: include or exclude item

    # Particle Swarm Optimization (PSO) for Evolutionary Swarm Intelligence
    num_particles = 10
    w = 0.5  # Inertia weight
    c1 = 1.5  # Cognitive parameter
    c2 = 1.5  # Social parameter

    # Initialize swarm
    particles = np.random.randint(0, 2, (num_particles, prize.shape[0]))

    # Define the position and velocity of particles
    velocities = np.zeros((num_particles, prize.shape[0]))
    positions = np.array(particles)

    # Define global best and personal best
    global_best_position = particles[np.argmax(Q_table.sum(axis=1))]
    personal_best_position = particles

    for iteration in range(100):  # Placeholder for the number of iterations
        # Update velocities
        velocities = w * velocities + c1 * np.random.randn(num_particles, prize.shape[0]) * (personal_best_position - positions) + c2 * np.random.randn(num_particles, prize.shape[0]) * (global_best_position - positions)

        # Update positions
        positions += velocities

        # Update personal and global best
        for i in range(num_particles):
            if np.sum(positions[i, :]) > 0:  # Ensure feasibility
                current_score = np.sum(prize[positions[i, :] == 1])
                if current_score > np.sum(prize[global_best_position == 1]):
                    global_best_position = positions[i, :]
                    Q_table[global_best_position == 1] = current_score

        # Update personal best
        for i in range(num_particles):
            current_score = np.sum(prize[positions[i, :] == 1])
            if current_score > np.sum(prize[personal_best_position[i] == 1]):
                personal_best_position[i] = positions[i, :]

    # Adaptive Constraint-Driven Filtering to Maintain Multi-Dimensional Feasibility
    for i in range(prize.shape[0]):
        total_prize = 0
        total_weight = np.zeros(m)
        for j in range(i, prize.shape[0]):
            total_weight += weight[j]
            if np.any(total_weight > 1):
                break
            total_prize += prize[j]
        heuristic[i] = total_prize

    # Sparsify Heuristics by Setting Unpromising Elements to Zero
    # This step can be combined with the RL and PSO to further refine heuristics
    # Placeholder for demonstration purposes
    for i in range(prize.shape[0]):
        if heuristic[i] < np.mean(heuristic):
            heuristic[i] = 0

    return heuristic
```
