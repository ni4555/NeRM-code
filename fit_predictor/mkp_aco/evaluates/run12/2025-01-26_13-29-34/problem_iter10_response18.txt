```python
import numpy as np

def heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:
    n = prize.shape[0]
    heuristic = np.zeros((n,))

    # Deep reinforcement learning component (sophisticated approach with exploration and exploitation)
    model_scores = np.random.rand(n)  # Hypothetical pre-trained model scores
    epsilon = 0.2  # Exploration rate
    alpha = 0.1  # Learning rate

    # Adaptive constraint-driven filtering (with sparsity and early termination)
    feasible_items = np.zeros(n, dtype=bool)
    for i in range(n):
        if np.any(weight[i, :] > 1):
            feasible_items[i] = False
            break
        else:
            feasible_items[i] = True

    # Particle swarm optimization (enhanced version with velocity and position update)
    global_best_index = np.argmax(model_scores)
    global_best_score = model_scores[global_best_index]
    pso_velocity = np.zeros(n)
    pso_position = np.copy(model_scores)
    pso_personal_best = np.copy(model_scores)
    pso_global_best = np.copy(model_scores)

    for _ in range(5):  # Using a smaller number of iterations for better adaptability
        for i in range(n):
            r1, r2 = np.random.rand(2)
            cognitive_factor = 2 * r1 * (pso_personal_best[i] - model_scores[i])
            social_factor = 2 * r2 * (pso_global_best[i] - model_scores[i])
            pso_velocity[i] = 0.5 * pso_velocity[i] + cognitive_factor + social_factor
            pso_position[i] += pso_velocity[i]
            if not feasible_items[i] or pso_position[i] < 0:
                pso_position[i] = model_scores[i]
        pso_personal_best = np.maximum(pso_position, pso_personal_best)
        pso_global_best = np.maximum(pso_personal_best, pso_global_best)
        global_best_index = np.argmax(pso_global_best)

    # Update heuristics based on model scores, PSO scores, and feasibility
    for i in range(n):
        score = (model_scores[i] + epsilon * (np.random.rand() - 0.5)) * (prize[i] / (prize[global_best_index] + 1e-6))
        if feasible_items[i]:
            heuristic[i] = score + (pso_global_best[i] * (1 - model_scores[i]))
        else:
            heuristic[i] = 0

        # Adaptive weight adjustments based on heuristic value
        if heuristic[i] < global_best_score * 0.5:
            heuristic[i] *= 0.9

    # Integrate domain knowledge to enhance heuristics
    # Incorporate item importance and sparsity
    importance_factor = prize / np.sum(prize[feasible_items])
    heuristic *= importance_factor

    # Dynamically adjust heuristics based on model confidence
    model_confidence = np.abs(model_scores - np.percentile(model_scores, 25))
    heuristic *= model_confidence

    # Sparsify heuristics by setting less promising elements to zero
    sparsity_threshold = np.percentile(heuristic[feasible_items], 50)
    heuristic[heuristic < sparsity_threshold] = 0

    return heuristic
```
