```python
import numpy as np

def heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:
    m = weight.shape[1]
    heuristic = np.zeros((prize.shape[0],))

    # Deep reinforcement learning component (simplified)
    # Assuming a pre-trained model that scores items based on their potential
    # Here we simulate this with a random number generator for demonstration
    model_scores = np.random.rand(prize.shape[0])

    # Adaptive constraint-driven filtering
    for i in range(prize.shape[0]):
        total_weight = np.sum(weight[i])
        if total_weight > 1:
            continue
        
        # Calculate cumulative weight and prize until constraint is violated
        cumulative_weight = np.zeros_like(prize)
        cumulative_prize = np.zeros_like(prize)
        cumulative_weight[i] = weight[i]
        cumulative_prize[i] = prize[i]
        for j in range(i + 1, prize.shape[0]):
            if np.sum(cumulative_weight[j - 1]) + np.sum(weight[j]) > 1:
                break
            cumulative_weight[j] = cumulative_weight[j - 1] + weight[j]
            cumulative_prize[j] = cumulative_prize[j - 1] + prize[j]

        # Particle swarm optimization for evolutionary swarm intelligence
        # Simulate the local best and global best positions
        local_best_prize = np.max(cumulative_prize[i:])
        global_best_prize = np.max(cumulative_prize)

        # Calculate heuristic based on the factors
        heuristic[i] = model_scores[i] * (prize[i] / (local_best_prize + 1e-6)) * ((global_best_prize - prize[i]) / (global_best_prize + 1e-6))

    # Sparsify the heuristics by setting unpromising elements to zero
    heuristic[heuristic < 0.5] = 0

    return heuristic
```
