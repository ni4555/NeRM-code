```python
import numpy as np

def heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:
    m = weight.shape[1]
    heuristic = np.zeros((prize.shape[0],))
    
    # Initialize variables for the sum of prizes and weights
    total_prize = np.zeros(prize.shape[0])
    total_weight = np.zeros(prize.shape[0])
    
    # Deep reinforcement learning component (simplified)
    # Assuming a pre-trained model that scores items based on their potential
    # Here we simulate this with a random number generator for demonstration
    model_scores = np.random.rand(prize.shape[0])
    
    # Adaptive constraint-driven filtering
    for i in range(prize.shape[0]):
        # Initialize the total weight and prize for the current item
        current_total_weight = weight[i].sum()
        current_total_prize = prize[i]
        
        # Update the total weight and prize for all items up to the current one
        for j in range(i + 1, prize.shape[0]):
            combined_weight = current_total_weight + weight[j].sum()
            if combined_weight > 1:
                break
            current_total_weight = combined_weight
            current_total_prize += prize[j]
        
        # Particle swarm optimization for evolutionary swarm intelligence
        # Simulate the local best and global best positions
        local_best = np.max(prize[i:])
        global_best = np.max(prize)
        
        # Calculate heuristic based on the factors
        heuristic[i] = model_scores[i] * (current_total_prize / (local_best + 1e-6)) * ((global_best - current_total_prize) / (global_best + 1e-6))
        
        # Store the total prize and weight for future reference
        total_prize[i] = current_total_prize
        total_weight[i] = current_total_weight
    
    # Sparsify the heuristics by setting unpromising elements to zero
    heuristic[heuristic < 0.5] = 0
    
    return heuristic
```
