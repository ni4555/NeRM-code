```python
import numpy as np

def heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:
    n = prize.shape[0]
    m = weight.shape[1]
    heuristic = np.zeros((n,))

    # Initialize a Deep Reinforcement Learning (DRL) model with a random policy
    model_policy = np.random.rand(n)

    # Initialize a Particle Swarm Optimization (PSO) algorithm
    pso_particles = np.random.rand(n, n)
    pso_particles = np.clip(pso_particles, 0, 1)
    pso_velocity = np.zeros((n, n))
    pso_best_position = np.copy(pso_particles)
    pso_global_best_position = np.copy(pso_particles[0])

    # Initialize a simple reward function for DRL
    def reward_function(policy, weight):
        return np.sum(policy * np.log(1 + policy * np.sum(weight, axis=1)))

    # Initialize a fitness function for PSO
    def fitness_function(position, weight):
        return reward_function(position, weight)

    # Define PSO parameters
    w = 0.5  # Inertia weight
    c1 = 2.0  # Cognitive parameter
    c2 = 2.0  # Social parameter

    # Run PSO for a few iterations
    for _ in range(10):
        # Update the velocity and position of the particles
        for i in range(n):
            r1, r2 = np.random.rand(2)
            pso_velocity[i] = (w * pso_velocity[i] +
                               c1 * r1 * (pso_best_position[i] - pso_particles[i]) +
                               c2 * r2 * (pso_global_best_position - pso_particles[i]))
            pso_particles[i] += pso_velocity[i]
            pso_particles[i] = np.clip(pso_particles[i], 0, 1)

        # Update the best and global best positions
        for i in range(n):
            if fitness_function(pso_particles[i], weight) > fitness_function(pso_best_position[i], weight):
                pso_best_position[i] = pso_particles[i]
            if fitness_function(pso_best_position[i], weight) > fitness_function(pso_global_best_position, weight):
                pso_global_best_position = pso_best_position[i]

    # Update the DRL policy with the best position from PSO
    model_policy = pso_global_best_position

    # Calculate the heuristics based on the policy and reward function
    for i in range(n):
        heuristic[i] = reward_function(model_policy, weight[:, i])

    # Sparsify the heuristics by setting a threshold
    threshold = np.percentile(heuristic, 50)
    heuristic[heuristic < threshold] = 0

    # Apply constraint-driven filtering to maintain feasibility
    feasible_items = np.sum(weight, axis=1) <= 1
    heuristic[~feasible_items] = 0

    return heuristic
```
