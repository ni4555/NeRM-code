```python
import numpy as np

def heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:
    m = weight.shape[1]
    heuristic = np.zeros((prize.shape[0],))
    rl_component = np.random.rand(prize.shape[0])
    pso_component = np.zeros((prize.shape[0],))
    
    # Reinforcement Learning component: Use a random walk to simulate the RL decision-making process
    for i in range(prize.shape[0]):
        max_reward = 0
        best_item = 0
        total_weight = 0
        for j in range(i, prize.shape[0]):
            total_weight += weight[j][:]
            if total_weight > 1:
                break
            reward = prize[j] - total_weight
            if reward > max_reward:
                max_reward = reward
                best_item = j
        heuristic[i] = max_reward + rl_component[i]
    
    # Particle Swarm Optimization component: Evaluate each particle's position based on the fitness function
    # Note: This is a simplified PSO component for illustrative purposes
    for i in range(prize.shape[0]):
        pso_component[i] = heuristic[i] * np.exp(-np.linalg.norm(weight[i] - np.random.rand(m)))
    
    # Adaptive constraint-driven filtering: Filter out items that are not feasible
    feasible_items = np.where(np.sum(weight, axis=1) <= 1)[0]
    heuristic[~feasible_items] = 0
    
    # Combine RL, PSO, and domain-specific heuristics
    combined_heuristic = (rl_component + pso_component + heuristic) / 3
    
    # Sparsify heuristics: Set unpromising elements to zero
    sparsified_heuristic = np.where(combined_heuristic > np.median(combined_heuristic), combined_heuristic, 0)
    
    return sparsified_heuristic
```
