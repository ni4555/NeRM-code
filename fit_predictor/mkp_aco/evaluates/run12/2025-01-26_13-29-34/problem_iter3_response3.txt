```python
import numpy as np

def heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:
    m = weight.shape[1]
    heuristic = np.zeros((prize.shape[0],))
    state = np.zeros((prize.shape[0], prize.shape[0]))

    # Deep Reinforcement Learning component (simplified)
    # Initialize Q-table with random values
    Q = np.random.rand(prize.shape[0], prize.shape[0])

    # Particle Swarm Optimization component
    # Initialize swarm
    swarm = np.random.rand(prize.shape[0], 2) * 100  # Random positions in range [0, 100]
    velocities = np.random.rand(prize.shape[0], 2) * 2 - 1  # Random velocities in range [-1, 1]

    # Adaptive constraint-driven filtering
    feasibility = np.zeros((prize.shape[0],))
    for i in range(prize.shape[0]):
        feasibility[i] = 1
        for j in range(i):
            cumulative_weight = np.sum(weight[j])
            if cumulative_weight > 1:
                feasibility[i] = 0
                break

    # Main optimization loop
    for _ in range(100):  # Number of iterations can be adjusted
        # Update positions and velocities using PSO
        swarm += velocities
        velocities *= 0.9  # Decaying velocities
        velocities += np.random.rand(prize.shape[0], 2) * 0.1  # Add randomness

        # Update heuristics based on Q-table and feasibility
        for i in range(prize.shape[0]):
            for j in range(i + 1, prize.shape[0]):
                if feasibility[i] and feasibility[j]:
                    total_weight = np.sum(weight[i] + weight[j])
                    if total_weight <= 1:
                        Q[i][j] = prize[i] + prize[j]
                        state[i][j] = 1

        # Update heuristics based on Q-values
        for i in range(prize.shape[0]):
            total_prize = 0
            for j in range(i, prize.shape[0]):
                total_prize += Q[i][j] * state[i][j]
            heuristic[i] = total_prize

    return heuristic
```
