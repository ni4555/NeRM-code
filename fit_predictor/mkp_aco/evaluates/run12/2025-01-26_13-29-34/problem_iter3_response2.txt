```python
import numpy as np

def heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:
    m = weight.shape[1]
    heuristic = np.zeros((prize.shape[0],))
    reward_model = np.zeros((prize.shape[0], prize.shape[0]))

    # Deep RL Integration
    # Here we would integrate a deep reinforcement learning model, which is not feasible
    # in the scope of this simulation. Instead, we simulate a reward function for the sake of this example.
    def simulate_reward_model():
        for i in range(prize.shape[0]):
            for j in range(i, prize.shape[0]):
                total_prize = 0
                total_weight = 0
                for k in range(m):
                    total_weight += weight[j][k]
                if total_weight > 1:
                    break
                total_prize += prize[j]
                reward_model[i][j] = total_prize

    simulate_reward_model()

    # PSO-inspired heuristic calculation
    for i in range(prize.shape[0]):
        total_prize = 0
        for j in range(prize.shape[0]):
            total_weight = np.sum(weight[j])
            if total_weight > 1:
                break
            total_prize += prize[j]
        heuristic[i] = reward_model[i].mean() + total_prize / prize.shape[0]

    # Adaptive Constraint-Driven Filtering
    for i in range(prize.shape[0]):
        for j in range(i, prize.shape[0]):
            if np.sum(weight[j]) > 1:
                reward_model[i][j] = 0

    # Sparsify heuristics
    heuristic = heuristic * (heuristic > heuristic.mean())

    return heuristic
```
