```python
def heuristics_v2(distance_matrix: torch.Tensor, demands: torch.Tensor) -> torch.Tensor:

    n = distance_matrix.shape[0]
    total_capacity = demands.sum()
    cumulative_demand = torch.cumsum(demands, dim=0)

    # Define a unified penalty function
    def penalty_function(value, target):
        return -torch.log1p(torch.abs(value - target) - 1e-8)

    # Scale the distance matrix with the cumulative demand and total capacity
    distance_demand_scale = penalty_function(distance_matrix, 0) * (cumulative_demand / total_capacity)

    # Apply the capacity penalty and early return encouragement
    capacity_penalty = penalty_function(demands - total_capacity, 0) * (1 + (total_capacity - cumulative_demand) / total_capacity)

    # Implement overflow penalty to encourage early returns
    overflow_penalty = torch.where(
        cumulative_demand[:, None] + demands[None, :] > total_capacity,
        penalty_function((cumulative_demand[:, None] + demands[None, :]) - total_capacity, 0),
        torch.zeros_like(cumulative_demand)
    )

    # Use load balancing to encourage an even distribution of demand
    load_balance = penalty_function(cumulative_demand - total_capacity / n, 0)

    # Combine all penalties
    combined_penalties = distance_demand_scale + capacity_penalty + overflow_penalty + load_balance

    # Apply a scaling factor and ensure the heuristic is non-negative
    scaling_factor = 3.0
    heuristics = torch.clamp(scaling_factor * combined_penalties, min=0.0)

    # Prevent returning to the depot from the start to promote exploration
    depot_penalty = torch.eye(n) - torch.eye(n, k=-1)
    heuristics[depot_penalty] -= 1e8

    return heuristics
```
