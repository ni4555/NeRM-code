{
  "generation": 7,
  "description": "Develop an advanced heuristic for the Capacitated Vehicle Routing Problem (CVRP) that leverages normalization to homogenize demand and distance metrics. The heuristic employs a hybrid approach, initially using an Inverse Distance heuristic for customer assignment, then integrating a demand-penalty mechanism to deter overloading vehicles. To enhance route quality, the algorithm incorporates a Genetic Algorithm (GA) for iterative refinement, Simulated Annealing (SA) to evade local minima, and Ant Colony Optimization (ACO) to uncover optimal paths. The system must be equipped with a robust real-time rerouting capability that swiftly adjusts vehicle assignments and routes in response to evolving customer demands and vehicle capacities within a stringent response time limit. The primary objective is to minimize the total travel distance, adhere to capacity limits, and ensure a scalable and responsive solution for fluctuating CVRP scenarios.",
  "failed_solutions": [
    {
      "code": "import torch\nimport torch\n\ndef heuristics_v2(distance_matrix: torch.Tensor, demands: torch.Tensor) -> torch.Tensor:\n    # Normalize the distance matrix by the maximum distance to ensure all distances are within [0, 1]\n    max_distance = torch.max(distance_matrix)\n    normalized_distance_matrix = distance_matrix / max_distance\n\n    # Normalize the demands by the sum of demands to scale them between 0 and 1\n    sum_of_demands = torch.sum(demands)\n    normalized_demands = demands / sum_of_demands\n\n    # Calculate the inverse distance heuristic (IDH) which is a measure of the attractiveness of visiting a node\n    # Higher values indicate more attractive (shorter distance)\n    inverse_distance_heuristic = 1 / normalized_distance_matrix\n\n    # Integrate demand-penalty mechanism to deter overloading vehicles\n    # Negative values are introduced to avoid nodes with high demands\n    demand_penalty = -normalized_demands\n\n    # Combine the IDH and demand penalty to get the initial heuristic values\n    initial_heuristic = inverse_distance_heuristic + demand_penalty\n\n    # Normalize the initial heuristic values to be between 0 and 1\n    min_value = torch.min(initial_heuristic)\n    max_value = torch.max(initial_heuristic)\n    normalized_initial_heuristic = (initial_heuristic - min_value) / (max_value - min_value)\n\n    return normalized_initial_heuristic",
      "fitness": Infinity,
      "execution_success": false,
      "error": "Traceback (most recent call last):\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2/problems/cvrp_pomo/eval.py\", line 140, in <module>\n    avg_obj = main()\n              ^^^^^^\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2/problems/cvrp_pomo/eval.py\", line 94, in main\n    avg_obj = tester.run()\n              ^^^^^^^^^^^^\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2\\problems\\cvrp_pomo\\CVRPTester.py\", line 71, in run\n    score, aug_score = self._test_one_batch(batch_size)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2\\problems\\cvrp_pomo\\CVRPTester.py\", line 109, in _test_one_batch\n    self.model.pre_forward(reset_state)\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2\\problems\\cvrp_pomo\\CVRPModel.py\", line 52, in pre_forward\n    assert not torch.isnan(self.attention_bias).any()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n",
      "stdout_file": "coevolve\\generation_7\\stdout_3.txt",
      "code_file": "coevolve\\generation_7\\code_3.py"
    },
    {
      "code": "import torch\nimport torch\n\ndef heuristics_v2(distance_matrix: torch.Tensor, demands: torch.Tensor) -> torch.Tensor:\n    # Normalize the distance matrix\n    max_distance = torch.max(distance_matrix)\n    normalized_distance_matrix = distance_matrix / max_distance\n\n    # Calculate the inverse distance heuristic\n    inverse_distance_matrix = 1 / normalized_distance_matrix\n\n    # Normalize the demands\n    max_demand = torch.max(demands)\n    normalized_demands = demands / max_demand\n\n    # Calculate the demand-penalty matrix\n    demand_penalty_matrix = normalized_demands - 1\n\n    # Combine the inverse distance and demand-penalty heuristics\n    combined_heuristic_matrix = inverse_distance_matrix - demand_penalty_matrix\n\n    # Apply a slight positive offset to ensure non-zero weights\n    combined_heuristic_matrix = combined_heuristic_matrix + 0.001\n\n    return combined_heuristic_matrix",
      "fitness": Infinity,
      "execution_success": false,
      "error": "Traceback (most recent call last):\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2/problems/cvrp_pomo/eval.py\", line 140, in <module>\n    avg_obj = main()\n              ^^^^^^\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2/problems/cvrp_pomo/eval.py\", line 94, in main\n    avg_obj = tester.run()\n              ^^^^^^^^^^^^\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2\\problems\\cvrp_pomo\\CVRPTester.py\", line 71, in run\n    score, aug_score = self._test_one_batch(batch_size)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2\\problems\\cvrp_pomo\\CVRPTester.py\", line 109, in _test_one_batch\n    self.model.pre_forward(reset_state)\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2\\problems\\cvrp_pomo\\CVRPModel.py\", line 53, in pre_forward\n    assert not torch.isinf(self.attention_bias).any()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n",
      "stdout_file": "coevolve\\generation_7\\stdout_5.txt",
      "code_file": "coevolve\\generation_7\\code_5.py"
    },
    {
      "code": "import torch\nimport torch\n\ndef heuristics_v2(distance_matrix: torch.Tensor, demands: torch.Tensor) -> torch.Tensor:\n    n = distance_matrix.shape[0]\n    \n    # Inverse Distance Heuristic (IDH)\n    inv_distance_heuristic = 1.0 / (distance_matrix + 1e-6)  # Add small constant to avoid division by zero\n    \n    # Demand-Penalty Heuristic\n    demand_penalty = -demands\n    \n    # Normalize the demand penalty to the same scale as the inverse distance heuristic\n    max_demand_penalty = torch.max(demand_penalty)\n    normalized_demand_penalty = demand_penalty / max_demand_penalty\n    \n    # Combine the heuristics\n    combined_heuristic = inv_distance_heuristic + normalized_demand_penalty\n    \n    # Apply a scaling factor to ensure that the heuristics are within a manageable range\n    scaling_factor = 1.0 / (torch.max(combined_heuristic) + 1e-6)\n    scaled_combined_heuristic = combined_heuristic * scaling_factor\n    \n    return scaled_combined_heuristic",
      "fitness": Infinity,
      "execution_success": false,
      "error": "Traceback (most recent call last):\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2/problems/cvrp_pomo/eval.py\", line 140, in <module>\n    avg_obj = main()\n              ^^^^^^\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2/problems/cvrp_pomo/eval.py\", line 94, in main\n    avg_obj = tester.run()\n              ^^^^^^^^^^^^\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2\\problems\\cvrp_pomo\\CVRPTester.py\", line 71, in run\n    score, aug_score = self._test_one_batch(batch_size)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2\\problems\\cvrp_pomo\\CVRPTester.py\", line 109, in _test_one_batch\n    self.model.pre_forward(reset_state)\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2\\problems\\cvrp_pomo\\CVRPModel.py\", line 52, in pre_forward\n    assert not torch.isnan(self.attention_bias).any()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n",
      "stdout_file": "coevolve\\generation_7\\stdout_6.txt",
      "code_file": "coevolve\\generation_7\\code_6.py"
    },
    {
      "code": "import torch\nimport torch\n\ndef heuristics_v2(distance_matrix: torch.Tensor, demands: torch.Tensor) -> torch.Tensor:\n    # Normalize the distance matrix by the maximum distance to ensure non-zero values\n    normalized_distance_matrix = distance_matrix / torch.max(distance_matrix)\n    \n    # Normalize the demands to be between 0 and 1\n    normalized_demands = demands / torch.sum(demands)\n    \n    # Calculate the inverse distance heuristic\n    inverse_distance_heuristic = 1 / normalized_distance_matrix\n    \n    # Integrate a demand-penalty mechanism\n    demand_penalty = 1 - demands\n    \n    # Combine the heuristics using a weighted sum\n    # The weights can be adjusted as needed\n    weight_inverse_distance = 0.7\n    weight_demand_penalty = 0.3\n    combined_heuristic = weight_inverse_distance * inverse_distance_heuristic + weight_demand_penalty * demand_penalty\n    \n    return combined_heuristic",
      "fitness": Infinity,
      "execution_success": false,
      "error": "Traceback (most recent call last):\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2/problems/cvrp_pomo/eval.py\", line 140, in <module>\n    avg_obj = main()\n              ^^^^^^\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2/problems/cvrp_pomo/eval.py\", line 94, in main\n    avg_obj = tester.run()\n              ^^^^^^^^^^^^\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2\\problems\\cvrp_pomo\\CVRPTester.py\", line 71, in run\n    score, aug_score = self._test_one_batch(batch_size)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2\\problems\\cvrp_pomo\\CVRPTester.py\", line 109, in _test_one_batch\n    self.model.pre_forward(reset_state)\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2\\problems\\cvrp_pomo\\CVRPModel.py\", line 53, in pre_forward\n    assert not torch.isinf(self.attention_bias).any()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n",
      "stdout_file": "coevolve\\generation_7\\stdout_7.txt",
      "code_file": "coevolve\\generation_7\\code_7.py"
    },
    {
      "code": "import torch\nimport torch\n\ndef heuristics_v2(distance_matrix: torch.Tensor, demands: torch.Tensor) -> torch.Tensor:\n    # Normalize demands by total vehicle capacity\n    normalized_demands = demands / demands.sum()\n    \n    # Calculate the inverse distance heuristic ((IDH) values)\n    idh_values = 1 / (distance_matrix + 1e-8)  # Adding a small value to avoid division by zero\n    \n    # Incorporate demand-penalty mechanism\n    demand_penalty = -10 * (normalized_demands[torch.arange(len(demands)), torch.arange(len(demands))] - 0.5)\n    \n    # Combine IDH and demand-penalty to get initial heuristics\n    initial_heuristics = idh_values + demand_penalty\n    \n    # Normalize heuristics to have non-negative values\n    initial_heuristics = (initial_heuristics - initial_heuristics.min()) / (initial_heuristics.max() - initial_heuristics.min())\n    \n    return initial_heuristics",
      "fitness": Infinity,
      "execution_success": false,
      "error": "Traceback (most recent call last):\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2/problems/cvrp_pomo/eval.py\", line 140, in <module>\n    avg_obj = main()\n              ^^^^^^\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2/problems/cvrp_pomo/eval.py\", line 94, in main\n    avg_obj = tester.run()\n              ^^^^^^^^^^^^\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2\\problems\\cvrp_pomo\\CVRPTester.py\", line 71, in run\n    score, aug_score = self._test_one_batch(batch_size)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2\\problems\\cvrp_pomo\\CVRPTester.py\", line 109, in _test_one_batch\n    self.model.pre_forward(reset_state)\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2\\problems\\cvrp_pomo\\CVRPModel.py\", line 49, in pre_forward\n    self.attention_bias = torch.stack([\n                                      ^\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2\\problems\\cvrp_pomo\\CVRPModel.py\", line 50, in <listcomp>\n    heuristics(distance_matrices[i], all_node_demands[i]) for i in range(all_nodes_xy.size(0))\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\ProgramLanguages\\Programs\\Python\\code-gen\\reevo_2\\problems\\cvrp_pomo\\gpt.py\", line 12, in heuristics_v2\n    demand_penalty = -10 * (normalized_demands[torch.arange(len(demands)), torch.arange(len(demands))] - 0.5)\n                            ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: too many indices for tensor of dimension 1\n",
      "stdout_file": "coevolve\\generation_7\\stdout_11.txt",
      "code_file": "coevolve\\generation_7\\code_11.py"
    }
  ]
}