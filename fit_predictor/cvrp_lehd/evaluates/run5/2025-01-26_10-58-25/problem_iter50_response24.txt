```python
import torch

def softmax(x, dim=1):
    x_exp = torch.exp(x - torch.max(x, dim=dim, keepdim=True)[0])
    return x_exp / x_exp.sum(dim=dim, keepdim=True)

def heuristics_v2(distance_matrix: torch.Tensor, demands: torch.Tensor) -> torch.Tensor:
    vehicle_capacity = demands.sum() / demands.numel()
    
    # Create a penalty matrix for demand constraint violations
    penalty_matrix = -torch.abs(demands - vehicle_capacity)
    
    # Create a distance-based penalty matrix to balance demand and distance penalties
    distance_penalty_matrix = -torch.log(distance_matrix + 1e-6)
    
    # Combine demand and distance penalties into a single potential matrix
    potential_matrix = penalty_matrix + distance_penalty_matrix
    
    # Apply a non-linear transformation to emphasize constraints
    emphasized_matrix = torch.exp(-torch.abs(potential_matrix))
    
    # Introduce a non-linear scaling factor to the emphasized matrix to further emphasize constraints
    scale_factor = 1 / (torch.sum(emphasized_matrix, dim=1, keepdim=True) + 1e-6)
    scaled_emphasized_matrix = emphasized_matrix * scale_factor
    
    # Normalize the scaled emphasized matrix to ensure non-negativity and scale balance
    normalized_emphasized_matrix = softmax(scaled_emphasized_matrix, dim=1)
    
    # Introduce a blending factor to combine the normalized matrix with the distance penalty matrix
    blending_factor = 0.6
    combined_matrix = (1 - blending_factor) * normalized_emphasized_matrix + blending_factor * distance_penalty_matrix
    
    # Adjust the combined matrix to ensure that the values are not too close to zero
    adjusted_combined_matrix = combined_matrix - torch.min(combined_matrix)
    
    # Normalize the adjusted combined matrix to ensure it sums to 1 over each row
    heuristics_matrix = adjusted_combined_matrix / adjusted_combined_matrix.sum(dim=1, keepdim=True)
    
    return heuristics_matrix
```
