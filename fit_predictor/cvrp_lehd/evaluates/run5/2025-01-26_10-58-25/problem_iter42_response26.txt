```python
import torch
from torch.nn.functional import log_softmax

def heuristics_v2(distance_matrix: torch.Tensor, demands: torch.Tensor) -> torch.Tensor:

    vehicle_capacity = demands.sum() / demands.numel()
    
    # Create a penalty matrix for demand constraint violations
    penalty_matrix = -torch.abs(demands - vehicle_capacity)
    
    # Create a distance-based penalty matrix to balance demand and distance penalties
    distance_penalty_matrix = -torch.log(distance_matrix + 1e-6)
    
    # Combine demand and distance penalties into a single potential matrix
    potential_matrix = penalty_matrix + distance_penalty_matrix
    
    # Apply a non-linear transformation to emphasize constraints
    emphasized_matrix = torch.tanh(potential_matrix)
    
    # Normalize the emphasized matrix to ensure non-negativity
    normalized_emphasized_matrix = torch.clamp(emphasized_matrix, min=0)
    
    # Scale the normalized emphasized matrix to the range [0, 1]
    scaled_emphasized_matrix = normalized_emphasized_matrix / normalized_emphasized_matrix.sum(dim=1, keepdim=True)
    
    # Combine the scaled emphasized matrix with the distance penalty matrix to balance the scales
    combined_matrix = (1 - 0.5) * scaled_emphasized_matrix + 0.5 * distance_penalty_matrix
    
    # Apply a soft-clamp to avoid negative values
    combined_matrix = torch.clamp(combined_matrix, min=0)
    
    # Normalize the combined matrix to ensure all values are between 0 and 1
    heuristics_matrix = log_softmax(combined_matrix, dim=1)
    
    return heuristics_matrix
```
