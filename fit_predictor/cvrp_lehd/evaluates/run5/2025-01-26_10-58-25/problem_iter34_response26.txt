```python
def softmax(x, dim):
    return torch.nn.functional.softmax(x, dim=dim)

def heuristics_v2(distance_matrix: torch.Tensor, demands: torch.Tensor) -> torch.Tensor:
    vehicle_capacity = demands.sum() / demands.numel()
    
    # Create a penalty matrix for demand constraint violations
    penalty_matrix = -torch.abs(demands - vehicle_capacity)
    
    # Create a distance-based penalty matrix to balance demand and distance penalties
    distance_penalty_matrix = -torch.log(distance_matrix + 1e-6)
    
    # Combine demand and distance penalties into a single potential matrix
    potential_matrix = penalty_matrix + distance_penalty_matrix
    
    # Apply an exponential decay for emphasis
    emphasized_matrix = torch.exp(-torch.abs(potential_matrix))
    
    # Normalize the emphasized matrix with softmax to balance scales
    normalized_emphasized_matrix = softmax(emphasized_matrix, dim=1)
    
    # Control the scale of the normalized matrix with clamping to avoid overflow and maintain stability
    adjusted_normalized_emphasized_matrix = torch.clamp(normalized_emphasized_matrix, min=0.1, max=10)
    
    # Combine the adjusted normalized emphasized matrix with the distance penalty matrix
    combined_matrix = adjusted_normalized_emphasized_matrix * penalty_matrix + distance_penalty_matrix
    
    # Ensure non-zero heuristic values by adding a small constant
    heuristics_matrix = combined_matrix + 1e-4
    
    # Transform the combined matrix into a heuristics matrix
    # Negative values represent undesirable edges, positive values represent promising ones
    heuristics_matrix = -heuristics_matrix
    
    # Normalize the heuristics matrix to ensure it sums to 1 over each row
    heuristics_matrix /= heuristics_matrix.sum(dim=1, keepdim=True)
    
    return heuristics_matrix
```
