```python
import torch
from torch.nn.functional import log_softmax, sigmoid, relu

def heuristics_v2(distance_matrix: torch.Tensor, demands: torch.Tensor) -> torch.Tensor:
    vehicle_capacity = demands.sum() / demands.numel()
    
    # Create a penalty matrix for demand constraint violations
    penalty_matrix = -torch.abs(demands - vehicle_capacity)
    
    # Create a distance-based penalty matrix to balance demand and distance penalties
    distance_penalty_matrix = -torch.log(distance_matrix + 1e-6)
    
    # Combine demand and distance penalties into a single potential matrix
    potential_matrix = penalty_matrix + distance_penalty_matrix
    
    # Apply ReLU to emphasize constraints while ignoring very negative values
    emphasized_matrix = torch.relu(potential_matrix)
    
    # Normalize the emphasized matrix to ensure non-negativity and scale balance
    max_emphasized, _ = torch.max(emphasized_matrix, dim=1, keepdim=True)
    normalized_emphasized_matrix = emphasized_matrix - max_emphasized
    log_softmax_output, _ = torch.log_softmax(normalized_emphasized_matrix, dim=1)
    normalized_emphasized_matrix = torch.exp(log_softmax_output)
    
    # Smooth the normalized emphasized matrix to avoid dominance
    smoothed_normalized_matrix = F.softmax(normalized_emphasized_matrix, dim=1)
    
    # Combine the smoothed normalized emphasized matrix with the distance penalty matrix
    combined_matrix = (1 - 0.5) * smoothed_normalized_matrix + 0.5 * distance_penalty_matrix
    
    # Ensure that the values are not too close to zero to represent edges
    combined_matrix = torch.relu(combined_matrix)
    
    # Transform the combined matrix into a heuristics matrix
    # Negative values represent undesirable edges, positive values represent promising ones
    heuristics_matrix = -combined_matrix
    
    return heuristics_matrix
```
