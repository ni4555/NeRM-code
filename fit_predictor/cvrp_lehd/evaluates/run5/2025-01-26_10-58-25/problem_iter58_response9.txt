```python
def softmax(x, dim):
    e_x = torch.exp(x - torch.max(x, dim=dim)[0])
    return e_x / e_x.sum(dim=dim, keepdim=True)

def heuristics_v2(distance_matrix: torch.Tensor, demands: torch.Tensor) -> torch.Tensor:
    vehicle_capacity = demands.sum() / demands.numel()
    
    # Create a penalty matrix for demand constraint violations
    penalty_matrix = -torch.abs(demands - vehicle_capacity)
    
    # Create a distance-based penalty matrix to balance demand and distance penalties
    distance_penalty_matrix = -torch.log(distance_matrix + 1e-6)  # Adding a small constant to avoid log(0)
    
    # Combine demand and distance penalties into a single potential matrix
    potential_matrix = penalty_matrix + distance_penalty_matrix
    
    # Emphasize the importance of the constraints by using a logarithmic transformation
    emphasized_matrix = torch.log(torch.exp(potential_matrix) + 1e-6)
    
    # Normalize the emphasized matrix to ensure non-negativity and scale balance
    normalized_emphasized_matrix = softmax(emphasized_matrix, dim=1)
    
    # Scale the normalized matrix with the distance penalties to emphasize distance
    scaled_normalized_matrix = normalized_emphasized_matrix * distance_penalty_matrix
    
    # Introduce a balancing factor to avoid dominance of one penalty over the other
    balancing_factor = torch.max(scaled_normalized_matrix) / distance_penalty_matrix
    balanced_scaled_normalized_matrix = scaled_normalized_matrix / balancing_factor
    
    # Introduce an offset to the adjusted combined matrix to ensure non-zero values
    offset = torch.min(balanced_scaled_normalized_matrix)
    adjusted_combined_matrix = balanced_scaled_normalized_matrix - offset
    
    # Transform the adjusted combined matrix into a heuristics matrix
    # Negative values represent undesirable edges, positive values represent promising ones
    heuristics_matrix = -adjusted_combined_matrix
    
    return heuristics_matrix
```
