```python
def heuristics_v2(distance_matrix: torch.Tensor, demands: torch.Tensor) -> torch.Tensor:

    n = distance_matrix.shape[0]
    total_capacity = demands.sum()
    scaled_demands = demands / total_capacity

    # Calculate cumulative demand from the depot to each node
    cumulative_demand = torch.cumsum(scaled_demands, dim=0)

    # Calculate remaining capacity at each node
    remaining_capacity = total_capacity - cumulative_demand

    # Introduce a dampening penalty for high cumulative demand to encourage earlier returns
    demand_penalty = torch.clamp(1.0 - remaining_capacity, min=0.0)

    # Normalize the distance matrix
    normalized_distance = distance_matrix / distance_matrix.max()

    # Use a non-linear transformation to balance the importance of distance and demand
    heuristics = demand_penalty * (1 - normalized_distance)

    # Introduce randomness for exploration and diversity
    exploration_noise = torch.randn_like(heuristics) * 0.01
    heuristics += exploration_noise

    # Apply a non-linear transformation to dampen the impact of high penalties
    heuristics = torch.tanh(heuristics)

    # Introduce a penalty for edges leading back to the depot
    depot_penalty = torch.zeros_like(distance_matrix)
    depot_penalty[torch.arange(n), 0] = -1
    heuristics += depot_penalty * 0.5

    # Normalize the heuristic values to encourage exploration
    max_heuristic = heuristics.max()
    min_heuristic = heuristics.min()
    heuristics = (heuristics - min_heuristic) / (max_heuristic - min_heuristic)

    # Introduce a preference for edges closer to the depot
    distance_preference = (normalized_distance - normalized_distance.mean()) * 10
    heuristics += torch.clamp(distance_preference, min=0.0) * 0.5

    # Clamp the heuristic values to ensure a balanced heuristic distribution
    heuristics = torch.clamp(heuristics, min=0.0, max=1.0)

    return heuristics
```
