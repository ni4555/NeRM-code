```python
def heuristics_v2(distance_matrix: torch.Tensor, demands: torch.Tensor) -> torch.Tensor:
    n = distance_matrix.shape[0]
    total_capacity = demands.sum()
    scaled_demands = demands / total_capacity

    cumulative_demand = torch.cumsum(scaled_demands, dim=0)
    remaining_capacity = total_capacity - cumulative_demand

    heuristics = (scaled_demands.unsqueeze(1) - scaled_demands.unsqueeze(0)).abs() * remaining_capacity

    dampening_factor = 0.5

    capacity_penalty = torch.clamp((scaled_demands > 1.0).float() * 1000.0, min=0.0)
    heuristics -= capacity_penalty * dampening_factor

    depot_penalty = torch.zeros_like(distance_matrix)
    depot_penalty[torch.arange(n), 0] = -1
    heuristics += depot_penalty * dampening_factor

    max_heuristic = heuristics.max()
    min_heuristic = heuristics.min()
    heuristics = (heuristics - min_heuristic) / (max_heuristic - min_heuristic)

    distance_penalty = (distance_matrix[:, 0] - distance_matrix[:, 0].mean()) * 30
    heuristics += torch.clamp(distance_penalty, min=0.0) * dampening_factor

    low_capacity_penalty = torch.exp(-0.5 * (remaining_capacity - remaining_capacity.mean()))
    heuristics += low_capacity_penalty * dampening_factor

    diversity_factor = torch.rand_like(heuristics)
    heuristics *= diversity_factor

    # Apply a non-linear transformation using a hyperbolic tangent
    heuristics = torch.tanh(heuristics)

    # Balance the penalties by scaling to the original range using min-max scaling
    heuristics = (heuristics - heuristics.min()) / (heuristics.max() - heuristics.min())

    # Introduce a multiplicative diversity factor to avoid premature convergence
    heuristics = heuristics * (torch.rand_like(heuristics) * 2 - 1)

    return heuristics
```
