```python
def heuristics_v2(distance_matrix: torch.Tensor, demands: torch.Tensor) -> torch.Tensor:
    n = distance_matrix.shape[0]
    total_capacity = demands.sum()
    scaled_demands = demands / total_capacity

    # Demand-driven sorting: higher demand nodes should be visited earlier
    sorted_indices = torch.argsort(scaled_demands, descending=True)

    # Precompute the cumulative demand and remaining capacity
    cumulative_demand = torch.cumsum(scaled_demands[sorted_indices], dim=0)
    remaining_capacity = total_capacity - cumulative_demand

    # Calculate the initial heuristics value based on sorted demand and remaining capacity
    heuristics = (scaled_demands[sorted_indices].unsqueeze(1) - scaled_demands[sorted_indices].unsqueeze(0)).abs() * remaining_capacity

    # Apply a dampening factor to balance the global view with local demand and distance
    dampening_factor = 0.5
    distance_dampened = distance_matrix * dampening_factor

    # Normalize the heuristics values to maintain balance and stability
    max_heuristic = heuristics.max()
    min_heuristic = heuristics.min()
    heuristics = (heuristics - min_heuristic) / (max_heuristic - min_heuristic)

    # Add penalties for infeasibility, dominated by the largest penalty
    infeasibility_penalty = (scaled_demands > 1.0).float() * 1000.0
    distance_penalty = (distance_dampened[:, 0] - distance_dampened[:, 0].mean()) * 100
    combined_penalties = torch.max(infeasibility_penalty, distance_penalty)
    heuristics = torch.where(combined_penalties > 0, combined_penalties, heuristics)

    # Apply penalties for edges leading back to the depot and for edges that are too far
    depot_penalty = torch.zeros_like(distance_matrix)
    depot_penalty[torch.arange(distance_matrix.shape[0]), 0] = -1
    far_penalty = (distance_dampened - distance_dampened[:, 0].unsqueeze(1)).abs() * 100
    combined_penalties = torch.max(depot_penalty, far_penalty)
    heuristics = torch.where(combined_penalties > 0, combined_penalties, heuristics)

    return heuristics
```
