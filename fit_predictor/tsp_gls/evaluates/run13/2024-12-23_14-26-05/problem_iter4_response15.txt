```python
import numpy as np

def heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:
    # Calculate the degree of each node
    degrees = np.sum(distance_matrix, axis=0)
    
    # Normalize the degrees to get a probability distribution
    probabilities = degrees / np.sum(degrees)
    
    # Adjust the probabilities based on centrality to avoid local minima
    # Using eigenvector centrality for this heuristic
    eigenvector_centrality = np.linalg.eigvals(np.dot(distance_matrix, distance_matrix.T))
    max_centrality = np.max(eigenvector_centrality)
    centrality_adjusted_probabilities = probabilities * (1 + 0.1 * np.log(1 + max_centrality / eigenvector_centrality))
    
    # Ensure probabilities are still normalized
    centrality_adjusted_probabilities /= np.sum(centrality_adjusted_probabilities)
    
    # Create a new distance matrix based on the adjusted probabilities
    new_distance_matrix = np.outer(centrality_adjusted_probabilities, centrality_adjusted_probabilities) * distance_matrix
    
    # Add a small constant to avoid zero distances
    epsilon = 1e-10
    new_distance_matrix = np.where(new_distance_matrix == 0, epsilon, new_distance_matrix)
    
    # Normalize the new distance matrix to keep the sum of distances constant
    total_distance = np.sum(new_distance_matrix)
    new_distance_matrix = new_distance_matrix / total_distance
    
    # Introduce non-linear adjustments to enhance diversity and escape local minima
    for i in range(new_distance_matrix.shape[0]):
        for j in range(new_distance_matrix.shape[0]):
            if new_distance_matrix[i, j] > 0:
                new_distance_matrix[i, j] = np.sqrt(new_distance_matrix[i, j]) + np.random.normal(0, 0.05)
    
    # Cap the distances to avoid unrealistic high values
    new_distance_matrix = np.clip(new_distance_matrix, 0, 1)
    
    return new_distance_matrix
```
